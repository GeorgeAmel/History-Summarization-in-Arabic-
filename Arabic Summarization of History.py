# -*- coding: utf-8 -*-
"""try (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c9Mz7ih5tm0aPYF--_go80_zsA1_PlrA
"""

import re

!pip install qalsadi

import qalsadi.lemmatizer

import pandas as pd

file_path = "/content/labeled_validation_dataset.jsonl"


df = pd.read_json(file_path, lines=True)

print(df)

# lemmer = qalsadi.lemmatizer.Lemmatizer()
# lemmer2 = qalsadi.lemmatizer.Lemmatizer()
# # Tokenize function
# def tokenize_text(row):
#     lemmer.set_vocalized_lemma()
#     lemmas = lemmer.lemmatize_text(row['paragraph'])
#     tokens1 = ' '.join(lemmas)
#     lemmer2.set_vocalized_lemma()
#     lemmas2 = lemmer2.lemmatize_text(row['summary'])
#     tokens2  = ' '.join(lemmas2)
#     return tokens1, tokens2

# # Apply tokenization function to DataFrame
# df[['Column1_Tokens', 'Column2_Tokens']] = df.apply(tokenize_text, axis=1, result_type='expand')

# # Display the updated DataFrame
# print(df)

!pip install mahad

# from maha.cleaners.functions import normalize

# def normalize_tokens(tokens, alef=True):
#     # Perform token normalization here
#     # Replace this with your own normalization logic
#     normalized_tokens = normalize(tokens, alef=False, all=True)
#     return normalized_tokens

# def maha_text(row):
#     paragraph_tokens = normalize_tokens(row['paragraph'])
#     summary_tokens = normalize_tokens(row['summary'])
#     return paragraph_tokens, summary_tokens

# df[['paragraph2', 'summary2']] = df.apply(maha_text, axis=1, result_type='expand')

# df = df.rename(columns={'paragraph': 'paragraph2', 'summary': 'summary2'})

# df

# from maha.cleaners.functions import remove
# def remove_tokens(tt):
#     # Perform token normalization here
#     # Replace this with your own normalization logic
#     rerere=remove(tt, punctuations=True)
#     return rerere

# def remove_punc(row):
#     pr = remove_tokens(row['paragraph2'])
#     sr = remove_tokens(row['summary2'])
#     return pr, sr

# df[['paragraph5', 'summary5']] = df.apply(remove_punc, axis=1, result_type='expand')

# df = df.rename(columns={'paragraph2': 'paragraph5', 'summary2': 'summary5'})

from sklearn.model_selection import train_test_split

train_texts, test_texts, train_summaries, test_summaries = train_test_split(df["paragraph"], df["summary"], test_size=0.2, random_state=42)

print("Number of training examples:", len(train_texts))
print("Number of testing examples:", len(test_texts))

pip install arabert

!pip install transformers

import re
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, AutoModelForSeq2SeqLM, pipeline
from arabert.preprocess import ArabertPreprocessor
from tensorflow import keras





model_name = "malmarjeh/mbert2mbert-arabic-text-summarization"
preprocessor = ArabertPreprocessor(model_name)

tokenizer = BertTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def create_lstm_model():
    model = keras.models.Sequential()
    model.add(keras.layers.LSTM(256, return_sequences=True, input_shape=(None, 768)))
    model.add(keras.layers.Dense(768, activation='softmax'))
    return model


lstm_model = create_lstm_model()
lstm_model.compile(optimizer='adam', loss='categorical_crossentropy')

summarizer = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

generated_summaries = []
for text in test_texts:
    preprocessed_text = preprocessor.preprocess(text)


    chunks = [preprocessed_text[i:i+512] for i in range(0, len(preprocessed_text), 512)]

    chunk_summaries = []
    for chunk in chunks:
        summary = summarizer(chunk, max_length=512, min_length=10, do_sample=True)[0]['generated_text']
        chunk_summaries.append(summary)

    combined_summary = " ".join(chunk_summaries)
    generated_summaries.append(combined_summary)

for summary in generated_summaries:
    print(summary)

import numpy as np

test_summaries=np.array(test_summaries)

import numpy as np

generated_summaries=np.array(generated_summaries)

pip install rouge

import nltk

nltk.download('punkt')

from rouge import Rouge
import nltk
from nltk.translate.bleu_score import sentence_bleu


rouge = Rouge()

rouge_scores = rouge.get_scores(generated_summaries, test_summaries, avg=True)

print("Rouge-L Score:", rouge_scores['rouge-l']['f'])

tokenized_generated_summaries = [nltk.word_tokenize(summary) for summary in generated_summaries]
tokenized_test_summaries = [nltk.word_tokenize(summary) for summary in test_summaries]

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer, util





model_name = 'distilbert-base-nli-stsb-mean-tokens'
model = SentenceTransformer(model_name)

generated_summaries = []
for text in test_texts:
    preprocessed_text = preprocessor.preprocess(text)

    chunks = [preprocessed_text[i:i+512] for i in range(0, len(preprocessed_text), 512)]

    chunk_summaries = []
    for chunk in chunks:
        summary = summarizer(chunk, max_length=512, min_length=10, do_sample=True)[0]['generated_text']
        chunk_summaries.append(summary)

    combined_summary = " ".join(chunk_summaries)
    generated_summaries.append(combined_summary)

similarities = []
for i in range(len(test_summaries)):
    similarity = util.pytorch_cos_sim(
        model.encode([test_summaries[i]]),
        model.encode([generated_summaries[i]])
    )
    similarities.append(similarity.item())

for i in range(len(generated_summaries)):
    print("True Summary:", test_summaries[i])
    print("Semantic Similarity:", similarities[i])
    print("------------")



similarities_array = np.array(similarities)

average_similarity = np.mean(similarities_array)
variance_similarity = np.var(similarities_array)

print("Average Semantic Similarity:", average_similarity)
print("Variance of Semantic Similarity:", variance_similarity)

